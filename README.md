

<img src="https://github.com/wildonion/hoopoe/blob/main/assets/hoopoe.png" width="300">

## ·ù∞.·êü What am i?

i'm hoopoe, the social event platform allows your hoop get heard!

## Execution flow & system design?

> [!TIP]
> any notification coming from different components or other service actor workers must be done accroding to the following steps:

- **step0)** a register notif api can be written to register either a producer or a consumer in the bakcground.

- **step1)** producer service actor sends `NotifData` to exchange.

- **step2)** consumer service actor receives `NotifData` from its queue bounded to the exchange.

- **step3)** instance of `NotifData` is cached on redis and stored in db.

- **step4)** client invokes `/notif/get/owner/` api to get its notification during the app execution in a short polling manner.

```

   ------------------ server1/node1 actor -----------------                                         ___________
  |                                                        |                   ____________________|           |
  |   ___________                            ___________   |                  |                    |           |
  |  |           |                          |           |  |    OVER WS STREAM/SHORT POLLING       |           |
  |  |           |                          |           |  |                  |                    |   CLIENT  |
  |  |  Actor1   |-----message handlers-----|  Actor2   |  |------------------------- HTTP --------|           |
  |  |  tokio    |     |___ jobq ___|       |   tokio   |  |             |                         |           |
  |  | threadpool|--rmq prodcons channels---| threadpool|  |             |                          -----------
  |  |           |                          |           |  |             |
  |   -----------                            -----------   |             |
   --------------------------------------------------------              |
  |  |                                                                   |
  |  |    synchronisation with                                           |
  |  |_____ rmq notif prodcons __                                        |
  |    data format:  NotifData   |    ----------------- server2/node2 actor ------------------
  |                              |   |                                                        |
  |                              |   |                                                        |
  |                              |   |   ___________                            ___________   |
  |                              |   |  |           |                          |           |  |
  |                              |___|  |           |                          |           |  |
  |        _________                 |  |  Actor1   |-----message handlers-----|  Actor2   |  |
  |       |         |                |  |  tokio    |     |___ jobq ___|       |  tokio    |  |
   -------|   PG    |                |  | threadpool|--rmq prodcons channels---| threadpool|  |
          |         |____mutators____|  |           |                          |           |  |
          |         |____readers ____|   -----------                            -----------   |
          |         |                |                                                        |
           ---------                  --------------------------------------------------------
```

<p align="center">
    <img src="https://github.com/wildonion/hoopoe/blob/main/infra/arch.png">
</p>

### proper way to write and handle async task execution! 

> [!IMPORTANT]
> every async task must be spawned in a free thread using a background worker which can be done via `tokio::spawn()`, it's like creating a worker per each async task.

```rust
async fn heavy_process(){
    // it can be one of the following tasks:
    // --> consuming task: send consume message to consumer actor
    // --> producing task: send produce message to producer actor
    // --> storing in db: send data to mutator actor 
    // --> storing in redis: cache on redis with exp key
    // --> locking logic 
}

let (tx, rx) = channel::<String>(); // channel for stringified data
let tx = tx.clone();
// spawn in the background and use channel to receive 
// data whenever the data is sent to the channel
tokio::spawn(async move{
    let res = heavy_process().await;
    tx.send(res).await;
});
while let Some(data) = rx.recv().await{
    // do the rest of the logics in here 
    // whenever we receive data
    // ...
}

// ------------
//      or 
// ------------

// spawn in the background but wait to gets solved 
// and once it's solved we then proceed with the rest
// of flow and cacnel other branches or async tasks
let task = tokio::spawn(async move{
    let res = heavy_process().await;
    tx.send(res).await;
});

tokio::select! {
    // choose this if it can gets completed soon
    _ = task => {
        // proceed with this flow
        // ...
    }, 
    // or if you think this gets completed soonly
    data = rx.recv() => {
        // proceed with this flow
        // ...
    }
}
```

### proper way to produce and consume data from RMQ broker

> [!IMPORTANT]
> start producing or consuming in the background by sending related message to their actors inside the `tokio::spawn()` scope, this way can be used to execute any async task or job gently in the background threads using tokio scheduler.

#### to produce data in the background: 

```rust
#[derive(Clone)]
struct SomeData{}
let data = SomeData{};
tokio::spawn( // running the producing notif job in the background in a free thread
    {
        let cloned_app_state = app_state.clone();
        let cloned_notif = ProduceNotif{
            "local_spawn": true,
            "notif_data": { 
                "receiver_info": "1",
                "id": "unqie-id0",
                "action_data": {
                    "pid": 200.4
                }, 
                "actioner_info": "2", 
                "action_type": "ProductPurchased", 
                "fired_at": 1714316645, 
                "is_seen": false
            }, 
            "exchange_name": "SavageEx",
            "exchange_type": "fanout", // amq.topic for pubsub
            "routing_key": "" // routing pattern or key - will be ignored if type is fanout
        };
        async move{
            match cloned_app_ctx.clone().unwrap().actors.as_ref().unwrap()
                    .broker_actors.notif_actor.send(cloned_notif).await
                {
                    Ok(_) => { 
                        
                        // in here you could access the notif for an owner using 
                        // a redis key like: notif_owner:3 which retrieve all data
                        // on the redis for the receiver with id 3   
                        () 

                    },
                    Err(e) => {
                        let source = &e.source().unwrap().to_string(); // we know every goddamn type implements Error trait, we've used it here which allows use to call the source method on the object
                        let err_instance = crate::error::HoopoeErrorResponse::new(
                            *MAILBOX_CHANNEL_ERROR_CODE, // error hex (u16) code
                            source.as_bytes().to_vec(), // text of error source in form of utf8 bytes
                            crate::error::ErrorKind::Actor(crate::error::ActixMailBoxError::Mailbox(e)), // the actual source of the error caused at runtime
                            &String::from("register_notif.producer_actor.notif_actor.send"), // current method name
                            Some(&zerlog_producer_actor)
                        ).await;
                        return;
                    }
                }
        }
);
```

> to consume data in the background:

```rust
tokio::spawn( // running the consuming notif job in the background in a free thread
    {
        let cloned_app_state = app_state.clone();
        let cloned_notif = ConsumeNotif{
            "queue": "TestOnion",
            "exchange_name": "SavageEx",
            "routing_key": "",
            "tag": "cons_tag0",
            "redis_cache_exp": 300,
            "local_spawn": true,
            "cache_on_redis": true,
            "store_in_db": true
        };
        async move{
            // consuming notif by sending the ConsumeNotif message to 
            // the consumer actor,
            match cloned_app_ctx.clone().unwrap().actors.as_ref().unwrap()
                    .broker_actors.notif_actor.send(cloned_notif).await
                {
                    Ok(_) => { () },
                    Err(e) => {
                        let source = &e.source().unwrap().to_string(); // we know every goddamn type implements Error trait, we've used it here which allows use to call the source method on the object
                        let err_instance = crate::error::HoopoeErrorResponse::new(
                            *MAILBOX_CHANNEL_ERROR_CODE, // error hex (u16) code
                            source.as_bytes().to_vec(), // text of error source in form of utf8 bytes
                            crate::error::ErrorKind::Actor(crate::error::ActixMailBoxError::Mailbox(e)), // the actual source of the error caused at runtime
                            &String::from("register_notif.consumer_actor.notif_actor.send"), // current method name
                            Some(&zerlog_producer_actor)
                        ).await;
                        return;
                    }
                }

        }
    }
);
```

## routes and apis

```bash
ü•õ HOOPOE WEBSOCKET STREAMING HTTP ROUTE   ==> https://event.api.hoopoe.app/stream
ü•õ HOOPOE WEBSOCKET STREAMING WS ROUTE     ==> wss://event.api.hoopoe.app/stream
üõ§Ô∏è HOOPOE HTTP APIs                        ==> https://api.hoopoe.app/
üõ§Ô∏è HOOPOE gRPC APIs                        ==> grpcs://grpc.api.hoopoe.app/
üõ¢Ô∏è HOOPOE ADMINER                          ==> https://adminer.hoopoe.app
üë®üèª‚Äçüíª HOOPOE DBEAVER                          ==> https://dbeaver.hoopoe.app
‚õµ HOOPOE PRTAINER                         ==> https://portainer.hoopoe.app
üìä HOOPOE GRAFANA                          ==> https://grafana.hoopoe.app
üö• HOOPOE RMQ                              ==> https://rmq.hoopoe.app
üóûÔ∏è HOOPOE LOGS                             ==> https://api.hoopoe.app/logs
üóÇÔ∏è HOOPOE ASSETS FOLDER                    ==> https://api.hoopoe.app/assets
üçÄ SWAGGER UI                              ==> https://api.hoopoe.app/swagger
üçÄ RAPIDOC UI                              ==> https://api.hoopoe.app/rapidoc
üçÄ SCALAR UI                               ==> https://api.hoopoe.app/scalar
üçÄ REDOC UI                                ==> https://api.hoopoe.app/redoc
```

## üóÉÔ∏è wikis, docs, erds, schemas and collections

[Rust Ownership and Borrowing Rules](https://github.com/wildonion/gvm/wiki/Ownership-and-Borrowing-Rules)

[ERD Schema](https://github.com/wildonion/hooper/blob/main/infra/hoopoe.png)

[HTTP Postman Collection](https://github.com/wildonion/hooper/blob/main/infra/api.http.json)

## How 2 setup, develop, and deploy?

> [!NOTE]
> if you want to deploy as a publisher or producer service then get your hands dirty by developing the `apis/http`, `actors/producers`, as a subscriber or consumer however, develop the `apis/http`, `actors/consumers` folders.

### Dev env

```bash
# -----------------------
# ---- read/write access
sudo chown -R root:root . && sudo chmod -R 777 . 
```

#### step0) install necessary packages on Linux:

```bash
cd scripts && ./setup.sh
```

#### step1) create database 

> make sure you've created the database using:

```bash
sqlx database create
```

#### step2) create migration folder (make sure you have it already!)

> make sure you uncomment the runtime setup inside its `Cargo.toml` file.

```bash
sea-orm-cli migrate init -d migration
```
#### step3) create migration files

> make sure you've installed the `sea-orm-cli` then create migration file per each table operation, contains `Migrations` structure with `up` and `down` methods extended by the `MigrationTrait` interface, take not that you must create separate migration per each db operation when you're in production.

```bash
sea-orm-cli migrate generate "sql_process_name_like_table_name_or_add_column"
```

#### step4) apply pending migrations (fresh, refresh, reset, up or down)

> once you've done with adding changes in your migration files just run the following to apply them in db.

```bash
# rollback all applied migrations, then reapply all migrations
sea-orm-cli migrate refresh # or up
```

#### step5) generate entity files for ORM operations

> generate Rust structures from your applied migrations inside the db for the `hoopoe` database in `entity/src` folder after that you can proceed with editing each eintity like adding hooks for different actions on an active model.

```bash
# generate entities for the database and all tables
sea-orm-cli generate entity -u postgres://postgres:geDteDd0Ltg2135FJYQ6rjNYHYkGQa70@localhost/hoopoe -o src/entities --with-serde both --serde-skip-deserializing-primary-key
# generate entity for an sepecific table only, eg: generating entity for hoops table
sea-orm-cli generate entity -t hoops -o src/entities --with-serde both --serde-skip-deserializing-primary-key
# don't skip deserializing primary key
sea-orm-cli generate entity -u postgres://postgres:geDteDd0Ltg2135FJYQ6rjNYHYkGQa70@localhost/hoopoe -o src/entities --with-serde both
```
#### step6) run server

> when you run server with `--fresh` command it'll fresh all migrations at startup (drop all tables from the database, then reapply all migrations) otherwise it'll only apply migrations (calling `up` method of all migration files).

```bash
# -------------------------------
# ------ hoopoe server --------
# -------------------------------
# launch as http with freshing db
cargo run --bin hoopoe -- --server http --fresh # default is http and fresh migrations
# or see help
cargo run --bin hoopoe -- --help
# ------------------------------
# ------ hooper servers --------
# ------------------------------
# launch as grpc with freshing db
cargo run --bin hooper -- --server grpc --fresh # default is grpc and fresh migrations
# launch as tcp with freshing db
cargo run --bin hooper -- --server tcp --fresh
# launch as p2p with freshing db
cargo run --bin hooper -- --server p2p --fresh
# or see help
cargo run --bin hooper -- --help
```

### üöÄ Prod env (the CI/CD approach):

> [!IMPORTANT]
> make sure you've opened all necessary domains inside your DNS panel per each nginx config file and changed the `hoopoe.app` to your own domain name in every where mostly the nginx config files and the `APP_NAME` in `consts.rs`. this approach can be used if you need a fully automatic deployment process, it uses github actions to build and publish all images on a self-hosted docker registry on a custom VPS, so update the github ci/cd workflow files inside `.github/workflows` folder to match your VPS infos eventually on every push the ci/cd process will begin to building and pushing automatically the docker images to the self-hosted registry. instead of using a custom registry you can use either ducker hub or github packages as well! it's notable that you should renew nginx service everytime you add a new domain or subdomain (do this on adding a new domain), `./renew.sh` script will create ssl certificates with certbot for your new domain and add it inside the `infra/docker/nginx` folder so nginx docker can copy them into its own container. for every new domain there must be its ssl certs and nginx config file inside that folder so make sure you've setup all your domains before pushing to the repo. continue reading... 

> you can build and up all images on your machine with `sudo docker compose up -d --build` command.

> [!IMPORTANT]
> if you want to run the server over **HTTP3** just make sure that ssl certificate keys are setup properly inside the `infra/http3/certs` folder, namely `hoopoecert.pem` and `hoopoekey.pem`.

#### üö® me before you!

> [!CAUTION]
> make sure you've done following configurations properly before pushing to your repo:

- **step0)** `sudo docker network create hoopoe_is_there1 && sudo docker network create hoopoe_is_there2` then create database with the same name inside the `.env` file on the VPS using command.

- **step1)** generate new ssl dh params for nginx using `openssl dhparam -out infra/docker/nginx/ssl-dhparams.pem 4096` command.

- **step2)** setup ssl certs using `renew.sh` script and nginx config files per each domain and subdomain then put them inside `infra/docker/nginx` folder, **you MUST do this before you get pushed to the repo on github** cause there is already an nginx container inside the `docker-compose.yml` needs its files to be there to move them into the container on every push! 

- **step3)** you would probably want to make `logs` dir and `docker.hoopoe.app` routes secure and safe, you can achieve this by adding an auth gaurd on the docker registry subdomain and the logs dir inside their nginx config files eventually setup the password for them by running `sudo apt-get install -y apache2-utils && sudo htpasswd -c infra/docker/nginx/.htpasswd hoopoe` command, the current one is `hoopoe@1234`.

- **step4)** run `sudo rm .env && sudo mv .env.prod .env` then update necessary variables inside `.env` file.

- **step5)** connect your device to github for workflow actions using `gh auth login -s workflow`, this allows you to push to the repo.

- **step6)** setup `DOCKER_PASSWORD`, `DOCKER_USERNAME`, `SERVER_HOST`, `SERVER_USER` and `SERVER_PASSWORD` secrets on your repository, _(`DOCKER_PASSWORD` and `DOCKER_USERNAME` are nginx login info to access the `docker.hoopoe.app` url)_.

- **step7)** created a `/root/hoopoe` folder on your VPS containing the `docker-compose.yml` file only and update its path inside the `cicd.yml` file in ssh action part where you're changing directory to where the docker compose file is in.

- **step8)** make sure the docker [registry](https://distribution.github.io/distribution/) service is up and running on your VPS (run `sudo docker run -d -p 5000:5000 --restart always --name registry registry:2`) and you have an already setup the `docker.hoopoe.app` subdomain for that which is pointing to the `http://localhost:5000`. use this command to run a registry docker: `sudo docker run -d -p 5000:5000 --restart always --name registry registry:2` then login to your hub with `sudo docker login docker.hoopoe.app` command (use the nginx username and password, note that if you've not setup a username and password no need to do the login process! simply run the push and pull on VPS).

- **step9)** each internal image name inside your compose file must be prefixed with your docker hub registry endpoint doing so tells docker to pull images from there cause as we know this subdoamin is already pointing to the docker registry hosted on `localhost:5000` on VPS.

> **current hub registry is set to `docker.hoopoe.app` and the `/root/hoopoe` folder on the VPS would be the place where the `docker-compose.yml` file is in**

> make sure you've logged in with `sudo` cause `cicd.yml` is building, pushing and pulling images with `sudo docker ...` command, if you are not running the docker with sudo make sure there is no sudo in `cicd.yml` file.

> [!TIP]
additionally you can push a docker image to your custom docker registry manually:
```bash
sudo docker login docker.hoopoe.app # login to the registry
sudo docker tag hoopoe-http docker.hoopoe.app/hoopoe-http # tag the image first
sudo docker push docker.hoopoe.app/hoopoe-http # push to the registry
```
login to the VPS and put the `docker-compose.yml` in the app directory (`/root/hoopoe`) then run:
```bash
cd /root/hoopoe/ && sudo docker compose -f "docker-compose.yml" up -d
```
automatically it'll pull the images from the specified registry in `image` key inside the compose file, just make sure that the image tag name is the one inside the `docker-compose.yml` file.

#### ‚òï What's happening inside the `cicd.yml` file?

- **step1)** it reads the codes inside the repository to find the `docker-compose.yml` file.

- **step1)** tries to login (docker username and password) to your custom docker hub (the registry on your VPS secured with nginx auth gaurd).

- **step2)** builda all docker container images inside your `docker-compose.yml` file.

- **step3)** eventually it pushes them to your custom docker hub registry.

- **step4)** does ssh to the VPS and cd to where you've put the `docker-compose.yml` file in there then pull and up all pushed docker containers from the VPS hub inside the VPS.

#### last but not least!

use postman or swagger ui to check the server health, continue with registering notif producer and consumer.